{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Introducción a Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "    ¿Qué aprenderá? \n",
    "    En este tutorial aprenderá sobre pyspark, dataframes, y persistencia\n",
    "\n",
    "    ¿Qué construirá? \n",
    "    En este tutorial se familiarizará con el uso del paquete Pyspark\n",
    "    \n",
    "    ¿Para qué?\n",
    "    Antes de realizar cualquier tipo de análisis, necesita conocimientos básicos sobre algunas tecnologías a usar a los largo del curso\n",
    "    \n",
    "    ¿Qué necesita?\n",
    "    Los siguientes requisitos se encuentran instalados en la máquina virtual asignada a cada estudiante, específicamente en el ambiente de anaconda llamado \"tutoriales\". Recuerde que tiene a su disposición el tutorial de conexión a máquinas virtuales en la semana 1 de Coursera\n",
    "    1. Python 3 con pip instalado\n",
    "    2. Jupyter notebook\n",
    "    3. Paquetes: Pyspark (3.0.1), pandas (1.2.1), numpy (1.20.0) y matplotlib (3.3.4)\n",
    "    Otros:\n",
    "    1. Controlador Connector J MySQL (ya se encuentra configurado)\n",
    "    2. Acceso a servidor remoto MySQL con base de datos relacional \"WWImportersTransactional\". Recuerde que tiene a su disposición el tutorial de conexión remoto a Mysql en la semana 1 de Coursera\n",
    "\n",
    "Para una contextualizacion general de pyspark dirijase al video tutorial <i>Sobre pyspark</i>\n",
    "\n",
    "La documentación de Pyspark es pública y la encuentra en: \n",
    "https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "Le recomendamos revisar la documentación y practicar dado que en los tutoriales no se cubriran todos los casos posibles de procesamiento de datos con Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuración e importe de paquetes\n",
    "Se importan los paquetes de python necesarios y se configura el controlador de conexion entre pyspark y mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import FloatType, StringType, IntegerType, DateType\n",
    "from pyspark.sql.functions import udf\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es una configuración necesaria para que el control del connector J de MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_jar_driver = 'C:\\Program Files (x86)\\MySQL\\Connector J 8.0\\mysql-connector-java-8.0.28.jar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear una sesión de PySpark \n",
    "\n",
    "Primero, aprenderá a crear una sesión de Spark. Las sesiones de Spark son el punto principal de entrada para programar Spark con Dataframes (estructuras de datos en forma de tabla), como se hará en la siguiente sección, pues presenta el API con el que se pueden manipular estos objetos. Para crear una sesión básica, basta con ejecutar el código a continuación:  \n",
    "\n",
    "``` \n",
    "spark = SparkSession.builder.appName('tutorial pyspark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sesión creada arriba tiene como nombre <i> tutorial pyspark </i>, está ejecutándose en la máquina local y está utilizando las configuraciones por defecto de PySpark. Para Coursera Labs NO se incluye esta configuración\n",
    "\n",
    "Aunque no será necesario para este curso, estas son algunas de las otras configuraciones que se pueden utilizar: \n",
    "\n",
    "\n",
    "\n",
    " - Usando la función <i>master</i> se puede configurar el clúster computacional sobre el que se ejecutará. Por ejemplo, si tuviera un clúster en la dirección 192.168.1.3, podría escoger ejecutar esta función de PySpark en ese clúster usando la función así:  \n",
    " \n",
    " ``` \n",
    " spark = SparkSession.builder.appName('tutorial pyspark').master('192.168.1.3').getOrCreate()\n",
    " \n",
    "- La función <i>enableHiveSupport</i> habilita el uso de Spark junto con Apache Hive y habilita las funcionalides de Hive:\n",
    "  ``` \n",
    "  spark = SparkSession.builder.appName('tutorial pyspark').master('192.168.1.3').enableHiveSupport().getOrCreate()\n",
    "\n",
    "- Finalmente, la función <i>config</i> permite configurar opciones más avanzadas, si así se desea:\n",
    "  ``` \n",
    "  spark = SparkSession.builder.appName('tutorial pyspark').master('192.168.1.3').enableHiveSupport().(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "  \n",
    "Recuerde que solo se puede tener una sesion de Spark activa al mismo tiempo, si ejecute varias veces la siguiente celda le saldra un error en relación a este hecho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Tutoriales\\lib\\site-packages\\pyspark\\sql\\context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "#Configuración de la sesión\n",
    "conf=SparkConf() \\\n",
    "    .set('spark.driver.extraClassPath', path_jar_driver)\n",
    "\n",
    "spark_context = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(spark_context)\n",
    "spark = sql_context.sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cerrar la sesión de Spark ejecute el comando <i>spark.stop()</i>. Recuerde que cualquier código que use la sesión spark no va a funcionar después de ejecutar el comando anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataframes\n",
    "\n",
    "Un Dataframe es una estructura de datos de PySpark que permite trabajar con datos estructurados. Un Dataframe organiza los datos en una tabla compuesta por columnas, de manera similar como se estructura una tabla SQL.\n",
    "\n",
    "Trabajar con un Dataframe permite utilizar el procesamiento distribuido de Spark de manera sencilla, sin tener que lidiar con las estructuras RDD de Spark, que son un poco más complicadas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen varias formas de crear un Dataframe, en este tutorial se verán tres:\n",
    "\n",
    "1. Crear un Dataframe programáticamente.\n",
    "2. Crear un Dataframe a partir de un archivo CSV.\n",
    "3. Crear un Dataframe a partir de una tabla de una base de datos relacional.\n",
    "\n",
    "La manipulación del dataframe en la etapa de preprocesamiento es independiente de la fuente de datos utilizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Dataframe programáticamente\n",
    "Crear un Dataframe programáticamente es simple, pues basta con utilizar el método <i>createDataFrame</i> de la sesión de Spark. El método puede inferir el esquema del Dataframe o se le puede especificar el esquema.\n",
    "\n",
    "En este ejemplo, especificamos un esquema con dos columnas <i>numeroEscrito</i>, de tipo <i>string</i>, y <i>numero</i>, de tipo <i>integer</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+------------+--------------+----------------+-------------+-------------------+--------+\n",
      "|ID_Cliente|              Nombre|ClienteFactura|ID_Categoria|ID_GrupoCompra|ID_CiudadEntrega|LimiteCredito|FechaAperturaCuenta|DiasPago|\n",
      "+----------+--------------------+--------------+------------+--------------+----------------+-------------+-------------------+--------+\n",
      "|         1|Tailspin Toys (He...|             1|           3|             1|           19586|            0|         2013-10-12|       7|\n",
      "+----------+--------------------+--------------+------------+--------------+----------------+-------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "esquema = StructType().add('ID_Cliente',IntegerType()).add('Nombre','string').add('ClienteFactura',IntegerType()).add('ID_Categoria',IntegerType()).add('ID_GrupoCompra',IntegerType()).add('ID_CiudadEntrega',IntegerType()).add('LimiteCredito',IntegerType()).add('FechaAperturaCuenta','date').add('DiasPago',IntegerType())\n",
    "   \n",
    "df_prog = spark.createDataFrame([(1,'Tailspin Toys (Head Office)',1,3,1,19586,0,datetime.strptime('2013-10-12','%Y-%m-%d'),7)], schema = esquema)\n",
    "\n",
    "#la función .show() del Dataframe, imprime las primeas 20 filas del mismo.\n",
    "df_prog.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Dataframe desde csv\n",
    "Para crear un Dataframe a partir de un archivo CSV se puede utilizar el método <i>load</i> del <i>DataFrameReader</i> de la sesión. A este método se le puede especificar la ruta del archivo (se recomienda usar variables ej. <i>PATH</i>), el tipo de archivo, el separador, si se desea que infiera el esquema o se le proveerá un esquema y si el archivo contiene encabezado o no. A continuación, se muestra un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------------+------------------+-------------+--------------+-----------+-----------------+-----------+\n",
      "|       _c0|                 _c1|             _c2|               _c3|          _c4|           _c5|        _c6|              _c7|        _c8|\n",
      "+----------+--------------------+----------------+------------------+-------------+--------------+-----------+-----------------+-----------+\n",
      "|CustomerID|        CustomerName|BillToCustomerID|CustomerCategoryID|BuyingGroupID|DeliveryCityID|CreditLimit|AccountOpenedDate|PaymentDays|\n",
      "|         1|Tailspin Toys (He...|               1|                 3|          1.0|         19586|       null|        1/01/2013|          7|\n",
      "|         2|Tailspin Toys (Sy...|               1|                 3|          1.0|         33475|       null|        1/01/2013|          7|\n",
      "|         3|Tailspin Toys (Pe...|               1|                 3|          1.0|         26483|       null|        1/01/2013|          7|\n",
      "|         4|Tailspin Toys (Me...|               1|                 3|          1.0|         21692|       null|        1/01/2013|          7|\n",
      "|         5|Tailspin Toys (Ga...|               1|                 3|          1.0|         12748|       null|        1/01/2013|          7|\n",
      "|         6|Tailspin Toys (Je...|               1|                 3|          1.0|         17054|       null|        1/01/2013|          7|\n",
      "|         7|Tailspin Toys (Fr...|               1|                 3|          1.0|         12152|       null|        1/01/2013|          7|\n",
      "|         8|Tailspin Toys (Bo...|               1|                 3|          1.0|          3673|       null|        1/01/2013|          7|\n",
      "|         9|Tailspin Toys (Ne...|               1|                 3|          1.0|         23805|       null|        1/01/2013|          7|\n",
      "|        10|Tailspin Toys (Wi...|               1|                 3|          1.0|         37403|       null|        1/01/2013|          7|\n",
      "|        11|Tailspin Toys (De...|               1|                 3|          1.0|          8987|       null|        1/01/2013|          7|\n",
      "|        12|Tailspin Toys (Bi...|               1|                 3|          1.0|          3081|       null|        1/01/2013|          7|\n",
      "|        13|Tailspin Toys (St...|               1|                 3|          1.0|         32887|       null|        1/01/2013|          7|\n",
      "|        14|Tailspin Toys (Lo...|               1|                 3|          1.0|         19908|       null|        1/01/2013|          7|\n",
      "|        15|Tailspin Toys (Ba...|               1|                 3|          1.0|          2111|       null|        1/01/2013|          7|\n",
      "|        16|Tailspin Toys (Co...|               1|                 3|          1.0|          7409|       null|        1/01/2013|          7|\n",
      "|        17|Tailspin Toys (Ea...|               1|                 3|          1.0|          9791|       null|        1/01/2013|          7|\n",
      "|        18|Tailspin Toys (Go...|               1|                 3|          1.0|         13304|       null|        1/01/2013|          7|\n",
      "|        19|Tailspin Toys (Le...|               1|                 3|          1.0|         19124|       null|        1/01/2013|          7|\n",
      "+----------+--------------------+----------------+------------------+-------------+--------------+-----------+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.load(PATH+\"Clientes.csv\", format=\"csv\", sep=\";\", inferSchema=\"true\")\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Carga de datos desde la base de datos relacional\n",
    "Cargar datos de una base de datos relacional es un poco más difícil, pues hay que asegurarse de tener el driver adecuado para la conexión, la dirección IP, usuario, contraseña y tabla de la base de datos. (se recomienda usar variables Ej. db_user, db_psswd, db_connection_string)\n",
    "\n",
    "Las bases de datos WWI_DWH_i hacen referencia a las de cada estudiante Estudiante_i, lo mismo sucede con las del proyecto Proyecto_DWH_I son Proyegto_Gi.\n",
    "\n",
    "![Infraestructura](./Arquitectura_infraestructura.png)\n",
    "\n",
    "Ahora, se cargará la tabla de clientes de Wide World Importers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = ''\n",
    "db_psswd = ''\n",
    "db_connection_string = 'jdbc:mysql://157.253.236.116:8080/WWImportersTransactional'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+------------+--------------+----------------+-------------+-------------------+--------+\n",
      "|ID_Cliente|              Nombre|ClienteFactura|ID_Categoria|ID_GrupoCompra|ID_CiudadEntrega|LimiteCredito|FechaAperturaCuenta|DiasPago|\n",
      "+----------+--------------------+--------------+------------+--------------+----------------+-------------+-------------------+--------+\n",
      "|         1|Tailspin Toys (He...|             1|           3|             1|           19586|         null|2013-01-01 00:00:00|       7|\n",
      "|         2|Tailspin Toys (Sy...|             1|           3|             1|           33475|         null|2013-01-01 00:00:00|       7|\n",
      "|         3|Tailspin Toys (Pe...|             1|           3|             1|           26483|         null|2013-01-01 00:00:00|       7|\n",
      "|         4|Tailspin Toys (Me...|             1|           3|             1|           21692|         null|2013-01-01 00:00:00|       7|\n",
      "|         5|Tailspin Toys (Ga...|             1|           3|             1|           12748|         null|2013-01-01 00:00:00|       7|\n",
      "|         6|Tailspin Toys (Je...|             1|           3|             1|           17054|         null|2013-01-01 00:00:00|       7|\n",
      "|         7|Tailspin Toys (Fr...|             1|           3|             1|           12152|         null|2013-01-01 00:00:00|       7|\n",
      "|         8|Tailspin Toys (Bo...|             1|           3|             1|            3673|         null|2013-01-01 00:00:00|       7|\n",
      "|         9|Tailspin Toys (Ne...|             1|           3|             1|           23805|         null|2013-01-01 00:00:00|       7|\n",
      "|        10|Tailspin Toys (Wi...|             1|           3|             1|           37403|         null|2013-01-01 00:00:00|       7|\n",
      "|        11|Tailspin Toys (De...|             1|           3|             1|            8987|         null|2013-01-01 00:00:00|       7|\n",
      "|        12|Tailspin Toys (Bi...|             1|           3|             1|            3081|         null|2013-01-01 00:00:00|       7|\n",
      "|        13|Tailspin Toys (St...|             1|           3|             1|           32887|         null|2013-01-01 00:00:00|       7|\n",
      "|        14|Tailspin Toys (Lo...|             1|           3|             1|           19908|         null|2013-01-01 00:00:00|       7|\n",
      "|        15|Tailspin Toys (Ba...|             1|           3|             1|            2111|         null|2013-01-01 00:00:00|       7|\n",
      "|        16|Tailspin Toys (Co...|             1|           3|             1|            7409|         null|2013-01-01 00:00:00|       7|\n",
      "|        17|Tailspin Toys (Ea...|             1|           3|             1|            9791|         null|2013-01-01 00:00:00|       7|\n",
      "|        18|Tailspin Toys (Go...|             1|           3|             1|           13304|         null|2013-01-01 00:00:00|       7|\n",
      "|        19|Tailspin Toys (Le...|             1|           3|             1|           19124|         null|2013-01-01 00:00:00|       7|\n",
      "|        20|Tailspin Toys (Co...|             1|           3|             1|            7160|         null|2013-01-01 00:00:00|       7|\n",
      "+----------+--------------------+--------------+------------+--------------+----------------+-------------+-------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bd = spark.read.format('jdbc')\\\n",
    "    .option('url', db_connection_string) \\\n",
    "    .option('dbtable', '''(SELECT * FROM Clientes) AS Compatible''') \\\n",
    "    .option('user', db_user) \\\n",
    "    .option('password', db_psswd) \\\n",
    "    .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
    "    .load()\n",
    "\n",
    "df_bd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, para persistir un dataframe en un csv se utiliza la librearía Pandas, específicamente el uso de df.toPandas().to_csv(RUTA_DESTINO) ingresando la ruta destino del nuevo archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El controlador que se está utilizando para conectarse a la base de datos es de tipo JDBC (Java Database Connectivity). Aunque no es necesario para este curso, si quiere saber más sobre JDBC puede consultar el siguiente recurso: \n",
    "\n",
    "https://www.infoworld.com/article/3388036/what-is-jdbc-introduction-to-java-database-connectivity.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar su comprensión de esta sección, lo invitamos a responder la pregunta:\n",
    "\n",
    "    ¿Qué es un DataFrame en PySpark, y cómo se puede crear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se crean dos funciones que retornan un dataframe desde una base de datos o desde un csv que le podrían ser utiles a futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obterner_dataframe_desde_csv(_PATH, _sep):\n",
    "    return spark.read.load(_PATH, format=\"csv\", sep=_sep, inferSchema=\"true\")\n",
    "\n",
    "def obtener_dataframe_de_bd(db_connection_string, sql, db_user, db_psswd):\n",
    "    df_bd = spark.read.format('jdbc')\\\n",
    "        .option('url', db_connection_string) \\\n",
    "        .option('dbtable', sql) \\\n",
    "        .option('user', db_user) \\\n",
    "        .option('password', db_psswd) \\\n",
    "        .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
    "        .load()\n",
    "    return df_bd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Select, where, filter....\n",
    "\n",
    "Los métodos <i>select</i> y <i>where</i> funcionan de manera similar a sus homónimos en SQL. De este modo, <i>select</i> servirá para seleccionar un subconjunto de columnas y <i>where</i> permitirá filtrar filas según los valores de sus columnas. Si lo prefiere, el método <i>filter</i> funciona igual que <i>where</i>.\n",
    "\n",
    "En el ejemplo que se muestra a continuación, se seleccionan las columnas <i>CustomerID</i>, <i>BuyingGroupID</i> y <i>CreditLimit</i>, tras lo cual se mantienen solamente aquellas filas cuyo <i>CreditLimit</i> no sea nulo y cuyo <i>CustomerID</i> sea mayor a 10.\n",
    "\n",
    "Recuerde que tiene a su disposición los recursos nivelatorios, entre los cuales se encuentra bases de datos relacionales y SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+\n",
      "|ID_Cliente|ID_GrupoCompra|LimiteCredito|\n",
      "+----------+--------------+-------------+\n",
      "|       801|          null|         3000|\n",
      "|       802|          null|         2940|\n",
      "|       803|          null|         2000|\n",
      "|       804|          null|         2200|\n",
      "|       805|          null|         3300|\n",
      "|       806|          null|         3000|\n",
      "|       807|          null|         3100|\n",
      "|       808|          null|         1800|\n",
      "|       809|          null|         1700|\n",
      "|       810|          null|         1200|\n",
      "|       811|          null|         2100|\n",
      "|       812|          null|         2200|\n",
      "|       813|          null|         2600|\n",
      "|       814|          null|         2310|\n",
      "|       815|          null|         2900|\n",
      "|       816|          null|         2400|\n",
      "|       817|          null|         2100|\n",
      "|       818|          null|         1155|\n",
      "|       819|          null|         1800|\n",
      "|       820|          null|         3500|\n",
      "+----------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta usando SQL\n",
    "sql_results = obtener_dataframe_de_bd(db_connection_string, '(SELECT ID_Cliente,ID_GrupoCompra,LimiteCredito FROM Clientes WHERE LimiteCredito IS NOT NULL AND ID_Cliente>10) AS Compatible', db_user, db_psswd)\n",
    "sql_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado anterior se puede replicar usando select, where o filter de PySpark de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+\n",
      "|ID_Cliente|ID_GrupoCompra|LimiteCredito|\n",
      "+----------+--------------+-------------+\n",
      "|       801|          null|         3000|\n",
      "|       802|          null|         2940|\n",
      "|       803|          null|         2000|\n",
      "|       804|          null|         2200|\n",
      "|       805|          null|         3300|\n",
      "|       806|          null|         3000|\n",
      "|       807|          null|         3100|\n",
      "|       808|          null|         1800|\n",
      "|       809|          null|         1700|\n",
      "|       810|          null|         1200|\n",
      "|       811|          null|         2100|\n",
      "|       812|          null|         2200|\n",
      "|       813|          null|         2600|\n",
      "|       814|          null|         2310|\n",
      "|       815|          null|         2900|\n",
      "|       816|          null|         2400|\n",
      "|       817|          null|         2100|\n",
      "|       818|          null|         1155|\n",
      "|       819|          null|         1800|\n",
      "|       820|          null|         3500|\n",
      "+----------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bd_small = df_bd.select('ID_Cliente','ID_GrupoCompra', 'LimiteCredito').where((df_bd['LimiteCredito'].isNotNull()) & (df_bd['ID_Cliente'] > 10))\n",
    "   \n",
    "df_bd_small.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando filter, el código sería: \n",
    "\n",
    "```\n",
    "df_bd_small = df_bd.select('ID_Cliente','ID_GrupoCompra', 'LimiteCredito').filter(df_bd['LimiteCredito'].isNotNull()).filter(df_bd['ID_Cliente'] > 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 User Defined Functions\n",
    "\n",
    "Aunque PySpark ofrece muchas funcionalidades, en ocasiones necesitará realizar operaciones más complejas o personalizadas, para este tipo de operaciones existen las UDF o User Defined Functions.\n",
    "Las UDF permiten aplicar funciones de Python creadas por usted mismo a las columnas de un DataFrame. En el ejemplo que se presenta a continuación, se normaliza la columna <i>CreditLimit</i> utilizando una UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para realizar una normalización min-max\n",
    "def normalizar(valor, minimo, maximo):\n",
    "    return (valor-minimo)/(maximo-minimo)\n",
    "\n",
    "#Se cambia el formato de la columna Limite Credito\n",
    "df_bd = df_bd.withColumn('LimiteCredito', df_bd['LimiteCredito'].cast(IntegerType()))\n",
    "#Se obtiene el CreditLimit mínimo\n",
    "min_cred = df_bd.agg({'LimiteCredito': 'min'}).collect()[0][0]\n",
    "#Se obtiene el CreditLimit máximo\n",
    "max_cred = df_bd.agg({'LimiteCredito': 'max'}).collect()[0][0]\n",
    "\n",
    "#Se define la función normalizar como una UDF \n",
    "#Se utiliza una función Lambda internamente para inyectar min_cred y max_cred pues por defecto una UDF solo puede recibir columnas por parámetro.\n",
    "norm_udf = udf(lambda x: normalizar(x, min_cred, max_cred), FloatType())\n",
    "\n",
    "#Primero se reemplazan los valores nulos, pues la función normalizar no puede operar con valores nulos.\n",
    "df_bd = df_bd.fillna({'LimiteCredito': 0.0})\n",
    "df_bd = df_bd.withColumn('LimiteCredito_normalizado', norm_udf(df_bd['LimiteCredito']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------------+\n",
      "|ID_Cliente|LimiteCredito|LimiteCredito_normalizado|\n",
      "+----------+-------------+-------------------------+\n",
      "|         1|            0|                     null|\n",
      "|         2|            0|                     null|\n",
      "|         3|            0|                     null|\n",
      "|         4|            0|                     null|\n",
      "|         5|            0|                     null|\n",
      "|         6|            0|                     null|\n",
      "|         7|            0|                     null|\n",
      "|         8|            0|                     null|\n",
      "|         9|            0|                     null|\n",
      "|        10|            0|                     null|\n",
      "|        11|            0|                     null|\n",
      "|        12|            0|                     null|\n",
      "|        13|            0|                     null|\n",
      "|        14|            0|                     null|\n",
      "|        15|            0|                     null|\n",
      "|        16|            0|                     null|\n",
      "|        17|            0|                     null|\n",
      "|        18|            0|                     null|\n",
      "|        19|            0|                     null|\n",
      "|        20|            0|                     null|\n",
      "+----------+-------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bd.select('ID_Cliente', 'LimiteCredito', 'LimiteCredito_normalizado').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que primero debemos definir la función de Python, tras lo cual se puede definir la udf y finalmente, aplicar la misma sobre el DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar su comprensión de esta sección, lo invitamos a responder las preguntas:\n",
    "\n",
    "    ¿Qué métodos podría utilizar para reemplazar los valores nulos de una columna por el promedio de la misma?\n",
    "    ¿Qué son las UDF y cómo funcionan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Persistencia en base de datos y en csv\n",
    "Para persistir los datos de un DataFrame en la base de datos es necesario indicar los datos a guardar, configurar la conexion con el servidor de base de datos, el nombre de la nueva tabla y las credenciales de autenticación. Para identificar los datos del dataframe a guardar se utilizan los métodos 'select' o 'selectExpr'. Posteriormente se configura la conexion al servidor indicando dirección ip, puerto y el nombre de la base de datos a la que se desea conectar. \n",
    "\n",
    "Tenga presente que la persistencia de un dataframe es independiente a la forma de con la cual se cargaron los datos (programática, con un archivo csv o desde una tabla en una base de datos). A continuación un ejemplo para el dataframe df_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertar en una nueva tabla\n",
    "def guardar_db(df, tabla, db_connection_string, db_user, db_psswd):\n",
    "    df.select('*').write.format('jdbc') \\\n",
    "      .mode('append') \\\n",
    "      .option('url', db_connection_string) \\\n",
    "      .option('dbtable', tabla) \\\n",
    "      .option('user', db_user) \\\n",
    "      .option('password', db_psswd) \\\n",
    "      .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
    "      .save()\n",
    "\n",
    "db_connection_string = 'jdbc:mysql://157.253.236.116:8080/Estudiante_43'\n",
    "guardar_db(df_bd, 'ClientesProcesados', db_connection_string, db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, para persistir un dataframe en un csv se utiliza la libreria pandas y el comando to_csv() ingresando la ruta destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_csv(df, _PATH):\n",
    "    df.toPandas().to_csv(_PATH)\n",
    "    \n",
    "guardar_csv(df_bd, PATH+'ClientesProcesados.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cierre\n",
    "\n",
    "Completado este tutorial ya sabrá la forma básica de utilizar PySpark. Ya sabe cómo crear DataFrames a partir de datos existentes, cómo seleccionar columnas o filas de este Dataframe y cómo aplicar sus propias funciones a estos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Información adicional\n",
    "\n",
    "Si quiere conocer más sobre PySpark la guía más detallada es la documentación oficial, la cual puede encontrar acá: https://spark.apache.org/docs/latest/api/python/index.html <br>\n",
    "Para ir directamente a la documentación de PySpark SQL, donde está la información sobre los DataFrames: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html <br>\n",
    "\n",
    "El Capítulo 2 del libro <i>Learn PySpark : Build Python-based Machine Learning and Deep Learning Models, New York: Apress. 2019</i> de Pramod Singh contiene muchos ejemplos útiles, puede encontrarlo en la biblioteca virtual de la universidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.Preguntas frecuentes\n",
    "\n",
    "1. En algunos casos, encontrará también información sobre <i>Pandas_UDF</i>. <i>Pandas_UDF</i> son también User Defined Functions, por lo general los Pandas UDF son más eficientes que los UDF tradicionales, sin embargo, hay un bug con la versión de PySpark y de Java que se está usando, lo que previene la utilización de Pandas_UDF.\n",
    "\n",
    "3. Si al ejecutar la configuración de la sesión Spark le aparece el error <i>Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*])</i> Reinicie el kernel y vuelva a ejecutar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
